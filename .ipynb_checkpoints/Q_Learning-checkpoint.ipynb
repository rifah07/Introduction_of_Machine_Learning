{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7e57843-789e-4b4a-82c5-426f5c88ff74",
   "metadata": {},
   "source": [
    "Q-learning is all about learning from interactions with an environment to achieve a specific goal. It's like teaching an agent (a robot, a software program, etc.) how to act optimally by trying different actions and seeing the outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b941ea65-ccd3-4445-af04-d0813af12763",
   "metadata": {},
   "source": [
    "Q-learning is a powerful tool that allows agents to learn optimal actions through trial and error, updating their knowledge based on feedback from the environment. It's widely used in robotics, game playing, and various AI applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b2bcc7-473d-45c4-b8b8-32ad07253e0f",
   "metadata": {},
   "source": [
    "Q-values: Metrics used to evaluate actions at specific states."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c2693d-d4e4-40a8-9c72-cf66a55c2645",
   "metadata": {},
   "source": [
    "Key Concepts:\n",
    "\n",
    "Agent: The entity that makes decisions (e.g., a robot).\n",
    "\n",
    "Environment: The world in which the agent operates (e.g., a grid, a maze).\n",
    "\n",
    "State (s): A specific situation or configuration of the environment (e.g., the agent's position in the grid).\n",
    "\n",
    "Action (a): Any possible move the agent can make (e.g., move left, right, up, down).\n",
    "\n",
    "Reward (r): The feedback from the environment after an action (e.g., +10 for reaching the goal, -1 for a step taken). Or, Positive or negative responses provided to the agent based on its actions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddfc6b2-7dcf-4418-907e-8f644a954e58",
   "metadata": {},
   "source": [
    "Step 1: Define the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e860b261-1298-411e-9440-9839af65ad7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0e2a23f-cc3a-40b4-bf54-a487134654db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the environment\n",
    "n_states = 16  # Number of states in the grid world\n",
    "n_actions = 4  # Number of possible actions (up, down, left, right)\n",
    "goal_state = 15  # Goal state\n",
    "\n",
    "# Initialize Q-table with zeros\n",
    "Q_table = np.zeros((n_states, n_actions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f160610-36b0-44b6-8937-c7b34f404f24",
   "metadata": {},
   "source": [
    "Step 2: Set Hyperparameters"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c541994e-c57b-400a-89ea-a4e1d2c81e2b",
   "metadata": {},
   "source": [
    "Hyperparameters are-\n",
    "1. Learning Rate (α):\n",
    "   Definition: Determines the extent to which new information overrides old information.\n",
    "   Function: Controls how quickly the Q-values are updated based on new experiences.\n",
    "   Range: Typically between 0 and 1. A lower value means slower learning, while a higher value leads to faster learning.\n",
    "   \n",
    "2. Discount Factor (γ):\n",
    "     Definition: Represents the importance of future rewards compared to immediate rewards.\n",
    "     Function: Balances the trade-off between short-term and long-term rewards.\n",
    "     Range: Also between 0 and 1. A value closer to 0 makes the agent focus on immediate rewards, while a value closer to 1 makes it consider future rewards more heavily.\n",
    "\n",
    "3. Exploration Rate (ε):\n",
    "   Definition: Determines the likelihood of the agent exploring new actions versus exploiting known ones.\n",
    "   Function: Controls the balance between exploration (trying new actions) and exploitation (using known actions that yield high rewards).\n",
    "   Range: Typically starts higher and gradually decreases. Common strategies include ε-greedy approaches where ε decreases over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b77294ec-adbd-4be7-9a60-b1e3169ff6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameters\n",
    "learning_rate = 0.85\n",
    "discount_factor = 0.96\n",
    "exploration_prob = 0.2\n",
    "epochs = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c937c625-fea2-4b3e-a43d-c260d17e1112",
   "metadata": {},
   "source": [
    "Step 3: Implement the Q-Learning Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bab55e4-97c1-4633-b715-7a9256eb71d1",
   "metadata": {},
   "source": [
    "Methods for Determining Q-Values-\n",
    "\n",
    "There are two methods for determining Q-values:\n",
    "\n",
    "Temporal Difference: Calculated by comparing the current state and action values with the previous ones.\n",
    "\n",
    "Bellman’s Equation: A recursive formula invented by Richard Bellman in 1957, used to calculate the value of a given state and determine its optimal position. It provides a recursive formula for calculating the value of a given state in a Markov Decision Process (MDP) and is particularly influential in the context of Q-learning and optimal decision-making."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ea27f31-04ed-403d-b2cc-8cd0c740f763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-learning algorithm\n",
    "for epoch in range(epochs):\n",
    "    current_state = np.random.randint(0, n_states)  # Start from a random state\n",
    "\n",
    "    while current_state != goal_state:\n",
    "        # Choose action with epsilon-greedy strategy\n",
    "        if np.random.rand() < exploration_prob:\n",
    "            action = np.random.randint(0, n_actions)  # Explore\n",
    "        else:\n",
    "            action = np.argmax(Q_table[current_state])  # Exploit\n",
    "\n",
    "        # Simulate the environment (move to the next state)\n",
    "        # For simplicity, move to the next state\n",
    "        next_state = (current_state + 1) % n_states\n",
    "\n",
    "        # Define a simple reward function (1 if the goal state is reached, 0 otherwise)\n",
    "        reward = 1 if next_state == goal_state else 0\n",
    "\n",
    "        # Update Q-value using the Q-learning update rule\n",
    "        Q_table[current_state, action] += learning_rate * \\\n",
    "            (reward + discount_factor *\n",
    "             np.max(Q_table[next_state]) - Q_table[current_state, action])\n",
    "\n",
    "        current_state = next_state  # Move to the next state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6890154b-2c10-4834-b695-865167a872d0",
   "metadata": {},
   "source": [
    "Step 4: Output the Learned Q-Table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc76bbe-c262-4227-994a-e0bf608459f3",
   "metadata": {},
   "source": [
    "A Q-table is a lookup table used in Q-learning, a type of reinforcement learning. It helps an agent decide the best action to take in a given state to maximize future rewards. Think of it as a big table of scores that guides the agent on which moves are the most beneficial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03b3cbbc-642e-44f3-8d3c-6387616b648e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned Q-table:\n",
      "[[0.53789767 0.56467331 0.55196816 0.        ]\n",
      " [0.58820137 0.58820122 0.58820136 0.58818795]\n",
      " [0.61270976 0.61270973 0.6127096  0.6127096 ]\n",
      " [0.63823933 0.63823933 0.63823931 0.63823931]\n",
      " [0.66483264 0.66483264 0.66483264 0.66483264]\n",
      " [0.692534   0.692534   0.692534   0.692534  ]\n",
      " [0.72138956 0.72138958 0.72138958 0.72138958]\n",
      " [0.75144748 0.75144748 0.75144748 0.75144748]\n",
      " [0.78275779 0.78275779 0.78275779 0.78275779]\n",
      " [0.8153727  0.8153727  0.8153727  0.8153727 ]\n",
      " [0.84934656 0.84934656 0.84934656 0.84934656]\n",
      " [0.884736   0.884736   0.884736   0.884736  ]\n",
      " [0.9216     0.9216     0.9216     0.9216    ]\n",
      " [0.96       0.96       0.96       0.96      ]\n",
      " [1.         1.         1.         1.        ]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# After training, the Q-table represents the learned Q-values\n",
    "print(\"Learned Q-table:\")\n",
    "print(Q_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
