{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# BiLSTMs (Bidirectional Long Short-Term Memory networks)\n",
        "\n",
        "BiLSTMs are a type of recurrent neural network (RNN) that processes sequence data in both forward and backward directions. This bidirectionality allows the model **to capture context from both past and future time steps**, which is especially useful for tasks like natural language processing (NLP), speech recognition, and more."
      ],
      "metadata": {
        "id": "Vkb5P0XXQUJS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Basic Concepts\n",
        "\n",
        "1. LSTMs:\n",
        "      * LSTMs are a type of RNN designed to handle long-term dependencies by mitigating the vanishing gradient problem using a memory cell and gates (input, forget, and output).\n",
        "\n",
        "2. Bidirectional:\n",
        "      * In a BiLSTM, two LSTM layers are used:\n",
        "          1. One processes the sequence from start to end (forward).\n",
        "          2. The other processes the sequence from end to start (backward).\n",
        "\n",
        "3. Output:\n",
        "      * The outputs from both directions are concatenated or combined at each time step, providing richer contextual information."
      ],
      "metadata": {
        "id": "V0W02HG9QeH_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is an example of how to implement a BiLSTM in PyTorch:"
      ],
      "metadata": {
        "id": "uUJB5fpKQxbf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0XovcRn_QPir"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the BiLSTM model\n",
        "class BiLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers=1, bidirectional=True):\n",
        "        super(BiLSTM, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.bidirectional = bidirectional\n",
        "        self.lstm = nn.LSTM(input_size,\n",
        "                            hidden_size,\n",
        "                            num_layers,\n",
        "                            bidirectional=bidirectional,\n",
        "                            batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size * 2 if bidirectional else hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Initialize hidden and cell states\n",
        "        h0 = torch.zeros(self.num_layers * (2 if self.bidirectional else 1), x.size(0), self.hidden_size)\n",
        "        c0 = torch.zeros(self.num_layers * (2 if self.bidirectional else 1), x.size(0), self.hidden_size)\n",
        "\n",
        "        # Forward propagate LSTM\n",
        "        out, _ = self.lstm(x, (h0, c0))  # out: (batch_size, seq_length, hidden_size * 2 if bidirectional)\n",
        "\n",
        "        # Pass the last hidden state to a fully connected layer\n",
        "        out = self.fc(out[:, -1, :])  # Take the last time-step output\n",
        "        return out"
      ],
      "metadata": {
        "id": "99_PXP5QReLm"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "input_size = 10    # Number of input features\n",
        "hidden_size = 20   # Number of hidden units\n",
        "output_size = 2    # Number of output classes\n",
        "num_layers = 2     # Number of stacked LSTM layers\n",
        "seq_length = 5     # Length of input sequence\n",
        "batch_size = 3     # Batch size"
      ],
      "metadata": {
        "id": "RxvT4M8vRh1u"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample input\n",
        "x = torch.randn(batch_size, seq_length, input_size)  # Shape: (batch_size, seq_length, input_size)\n",
        "\n",
        "# Model\n",
        "model = BiLSTM(input_size, hidden_size, output_size, num_layers)\n",
        "output = model(x)\n",
        "\n",
        "print(\"Output shape:\", output.shape)  # Expected: (batch_size, output_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJmMVpxhRreQ",
        "outputId": "683eec34-56bf-4610-d822-f030aa41f49c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: torch.Size([3, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explanation of the Code\n",
        "\n",
        "1. Model Initialization:\n",
        "      1. input_size: Number of input features per time step.\n",
        "      2. hidden_size: Number of hidden units in the LSTM.\n",
        "      3. output_size: Size of the output layer.\n",
        "      4. num_layers: Number of LSTM layers (stacked).\n",
        "      5. bidirectional: Enables bidirectionality.\n",
        "\n",
        "2. LSTM Layer:\n",
        "      * nn.LSTM is used with the bidirectional=True flag for BiLSTM functionality.\n",
        "\n",
        "3. Forward Pass:\n",
        "      * Initializes the hidden and cell states (h0, c0) with zeros.\n",
        "      * Processes the input sequence through the LSTM layer.\n",
        "      * The final output is passed through a fully connected layer (fc) to map it to the desired output size.\n",
        "\n",
        "4. Output:\n",
        "      * The model outputs the prediction for the last time step of the sequence."
      ],
      "metadata": {
        "id": "yruHeZn6RAIP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Applications of BiLSTMs\n",
        "\n",
        "* Text Classification: Captures both past and future context for better understanding.\n",
        "* Named Entity Recognition (NER): Considers surrounding words for accurate tagging.\n",
        "* Machine Translation: Improves alignment between input and output sequences.\n"
      ],
      "metadata": {
        "id": "X2WmoasJQ03_"
      }
    }
  ]
}