# -*- coding: utf-8 -*-
"""LSTM_HyperParam_Tuning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11_wRnMUbfgE7sSwI8WayxyTuub2Rz-zT
"""

import numpy as np
import torch
import torch.nn as nn
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# 1. Generate Sine Wave Dataset
def create_sine_wave_data(seq_length, n_samples):
    x = np.linspace(0, 50, n_samples)
    y = np.sin(x)
    data = []
    target = []
    for i in range(len(y) - seq_length):
        data.append(y[i:i + seq_length])
        target.append(y[i + seq_length])
    return np.array(data), np.array(target)

seq_length = 10  # Number of time steps
n_samples = 500  # Number of total points
X, y = create_sine_wave_data(seq_length, n_samples)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Convert to PyTorch tensors
X_train = torch.tensor(X_train, dtype=torch.float32).unsqueeze(-1)  # Add input_size=1
y_train = torch.tensor(y_train, dtype=torch.float32).unsqueeze(-1)
X_test = torch.tensor(X_test, dtype=torch.float32).unsqueeze(-1)
y_test = torch.tensor(y_test, dtype=torch.float32).unsqueeze(-1)

# 2. Define LSTM Model
class LSTMModel(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size):
        super(LSTMModel, self).__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        out, _ = self.lstm(x)  # LSTM layer
        out = self.fc(out[:, -1, :])  # Fully connected layer
        return out

# 3. Train and Evaluate Model
def train_and_evaluate_model(hidden_size, num_layers, learning_rate, epochs=50):
    input_size = 1  # Single feature
    output_size = 1  # Predict one value
    model = LSTMModel(input_size, hidden_size, num_layers, output_size)

    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

    for epoch in range(epochs):
        model.train()
        optimizer.zero_grad()
        output = model(X_train)
        loss = criterion(output, y_train)
        loss.backward()
        optimizer.step()

    # Evaluate on test set
    model.eval()
    with torch.no_grad():
        y_pred = model(X_test)
        test_loss = mean_squared_error(y_test.numpy(), y_pred.numpy())

    return test_loss

# 4. Hyperparameter Tuning (Grid Search)
hidden_sizes = [16, 32]
num_layers_list = [1, 2]
learning_rates = [0.001, 0.01]

best_params = None
best_loss = float('inf')

for hidden_size in hidden_sizes:
    for num_layers in num_layers_list:
        for learning_rate in learning_rates:
            print(f"Testing hidden_size={hidden_size}, num_layers={num_layers}, learning_rate={learning_rate}")
            test_loss = train_and_evaluate_model(hidden_size, num_layers, learning_rate)
            print(f"Test Loss: {test_loss:.4f}")

            if test_loss < best_loss:
                best_loss = test_loss
                best_params = (hidden_size, num_layers, learning_rate)

print("\nBest Parameters:")
print(f"Hidden Size: {best_params[0]}, Num Layers: {best_params[1]}, Learning Rate: {best_params[2]}")
print(f"Best Loss: {best_loss:.4f}")